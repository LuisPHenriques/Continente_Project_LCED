{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Install and Import Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41655,"status":"ok","timestamp":1683754027326,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"1l3_4sklgfVi","outputId":"e9ef4914-35b2-4cda-ee27-64bf29382d87"},"outputs":[],"source":["!pip install git+https://github.com/oracle/Skater.git"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3564,"status":"ok","timestamp":1683754030883,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"BDhC9ahTICDb"},"outputs":[],"source":["import pandas as pd\n","import heapq\n","import pickle\n","import numpy as np\n","import seaborn as sns\n","import random\n","import matplotlib.patches as mpatches\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","from datetime import time\n","import sklearn\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn import svm, datasets\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","#from tabulate import tabulate\n","from sklearn import cluster\n","from sklearn.metrics import silhouette_score\n","from sklearn.linear_model import Perceptron\n","from timeit import timeit\n","from sklearn import datasets, tree\n","import datetime\n","import os\n","import warnings\n","warnings.simplefilter(action = 'ignore', category=FutureWarning)\n","warnings.filterwarnings('ignore')\n","def ignore_warn(*args, **kwargs):\n","    pass\n","warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","import pylab \n","sns.set(style=\"ticks\", color_codes=True, font_scale=1.5)\n","from matplotlib import pyplot as plt\n","from matplotlib.ticker import FormatStrFormatter\n","from matplotlib.colors import ListedColormap\n","import matplotlib.colors as mcolors\n","%matplotlib inline\n","import mpl_toolkits\n","from mpl_toolkits.mplot3d import Axes3D\n","#from graphviz import Source\n","from IPython.display import Image\n","from scipy.stats import skew, norm, probplot, boxcox, f_oneway\n","from scipy import interp\n","from sklearn.base import BaseEstimator, TransformerMixin, clone, ClassifierMixin\n","from sklearn import metrics, tree\n","from sklearn.preprocessing import LabelEncoder, label_binarize, StandardScaler, PolynomialFeatures, MinMaxScaler\n","from imblearn.over_sampling import SMOTE, RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler\n","from sklearn.decomposition import PCA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, cross_val_predict, train_test_split, RandomizedSearchCV\n","from sklearn.feature_selection import SequentialFeatureSelector\n","from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","# from keras.models import Sequential\n","# from keras.layers import Dense\n","# from keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","#import xgboost as xgb\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.ensemble import GradientBoostingClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.svm import SVC, LinearSVC\n","from sklearn.cluster import DBSCAN\n","#from mlxtend.classifier import StackingClassifier\n","from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n","from skater.core.explanations import Interpretation\n","from skater.model import InMemoryModel\n","import scipy\n","from sklearn.model_selection import GridSearchCV\n","from google.cloud import bigquery\n","from sklearn.model_selection import TimeSeriesSplit\n","from scipy.stats import chi2_contingency\n","import joblib"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Loading the Data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1303,"status":"ok","timestamp":1683754081142,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"g3BbQ3ceIF3D"},"outputs":[],"source":["# set display options to show all columns\n","pd.set_option('display.max_columns', None)\n","# Set the float format to display numbers without scientific notation\n","pd.options.display.float_format = '{:.2f}'.format\n","# Set the client for future queries to BigQuery\n","client = bigquery.Client(project = \"continente-lced-feup\")\n","#data_table.enable_dataframe_formatter()\n","#auth.authenticate_user()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48971,"status":"ok","timestamp":1683754079843,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"-RODB9NDIF53","outputId":"f8217fef-ca72-4cb4-b0d6-a1aeb2a278eb"},"outputs":[],"source":["!gcloud auth application-default login"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":505},"executionInfo":{"elapsed":9109,"status":"ok","timestamp":1683754090249,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"yjHx6fYlIF8A","outputId":"5ae67c63-4db2-4d40-e29b-044ab14332d9"},"outputs":[],"source":["query = client.query(\"\"\"\n","   SELECT *\n","   FROM \n","       tables_staging.df_bold_model\n","  LIMIT 6000000\n","   \"\"\")\n","\n","df = query.result().to_dataframe() # Wait for the job to complete.\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["query_products = client.query(\"\"\"\n","   SELECT *\n","   FROM \n","       tables_raw.dim_product\n","  LIMIT 6000000\n","   \"\"\")\n","\n","pdct_df = query_products.result().to_dataframe() # Wait for the job to complete."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pdct_df = pdct_df[['SUBCAT_CD_EXT', 'SUBCAT_DSC_EXT']]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LaBORSavJiC_"},"source":["## Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12256,"status":"ok","timestamp":1683754102499,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"CiaXBFcOPyfH"},"outputs":[],"source":["\n","\n","# Specify the columns to drop null values except for\n","columns_to_exclude = ['TARGET', 'REG_AVG_DAYS_SINCE_PRIOR_TRANSACTION_MONTH']\n","\n","# Drop null values except for the specified columns\n","df = df.dropna(subset=[col for col in df.columns if col not in columns_to_exclude])\n","\n","\n","#df = df.dropna()\n","df = df.drop_duplicates()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","df['fulldate'] = pd.to_datetime(df['MONTH'].astype(str) + '-' + df['YEAR'].astype(str))\n","# change from dtype datetime64[ns] to date time month\n","df['fulldate'] = df['fulldate'].dt.to_period('M')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_500k = df.groupby('fulldate').sample(frac=0.2, random_state=101)\n","\n","#drop customer id column because we want to generalize the model, instead of trying to predict for each customer\n","df_500k = df_500k.drop(columns=['CUSTOMER_ACCOUNT_NR_MASK'])\n","\n","df_500k.value_counts('fulldate').sort_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":426,"status":"ok","timestamp":1683754102919,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"F5Z-ppPkIGB4"},"outputs":[],"source":["# numerical_columns = ['CUST_NUM_TRANSACTIONS_30_DAYS', 'CUST_NUM_TRANSACTIONS_90_DAYS',\n","#        'CUST_NUM_TRANSACTIONS_180_DAYS', 'CUST_NUM_TRANSACTIONS_360_DAYS',\n","#        'CUST_NUM_UNIQUE_SUBCAT_30_DAYS', 'CUST_NUM_UNIQUE_SUBCAT_90_DAYS',\n","#        'CUST_NUM_UNIQUE_SUBCAT_180_DAYS', 'CUST_NUM_UNIQUE_SUBCAT_360_DAYS',\n","#        'CUST_AVG_DAYS_FOR_NEXT_TRANSACTION_30_DAYS',\n","#        'CUST_AVG_DAYS_FOR_NEXT_TRANSACTION_90_DAYS',\n","#        'CUST_AVG_DAYS_FOR_NEXT_TRANSACTION_180_DAYS',\n","#        'CUST_AVG_DAYS_FOR_NEXT_TRANSACTION_360_DAYS',\n","#        'CUST_AVG_BASKET_SIZE_30_DAYS', 'CUST_AVG_BASKET_SIZE_90_DAYS',\n","#        'CUST_AVG_BASKET_SIZE_180_DAYS', 'CUST_AVG_BASKET_SIZE_360_DAYS',\n","#        'SUBCAT_NUM_TRANSACTIONS_30_DAYS', 'SUBCAT_NUM_TRANSACTIONS_90_DAYS',\n","#        'SUBCAT_NUM_TRANSACTIONS_180_DAYS', 'SUBCAT_NUM_TRANSACTIONS_360_DAYS',\n","#        'SUBCAT_NUM_UNIQUE_CUST_30_DAYS', 'SUBCAT_NUM_UNIQUE_CUST_90_DAYS',\n","#        'SUBCAT_NUM_UNIQUE_CUST_180_DAYS', 'SUBCAT_NUM_UNIQUE_CUST_360_DAYS',\n","#        'CUSTSUBCAT_NUM_TRANSACTIONS_30_DAYS',\n","#        'CUSTSUBCAT_NUM_TRANSACTIONS_90_DAYS',\n","#        'CUSTSUBCAT_NUM_TRANSACTIONS_180_DAYS',\n","#        'CUSTSUBCAT_NUM_TRANSACTIONS_360_DAYS',\n","#        'CUSTSUBCAT_AVG_DAYS_FOR_NEXT_TRANSACTION_30_DAYS',\n","#        'CUSTSUBCAT_AVG_DAYS_FOR_NEXT_TRANSACTION_90_DAYS',\n","#        'CUSTSUBCAT_AVG_DAYS_FOR_NEXT_TRANSACTION_180_DAYS',\n","#        'CUSTSUBCAT_AVG_DAYS_FOR_NEXT_TRANSACTION_360_DAYS']\n","\n","numerical_columns = ['CUST_NUM_TRANSACTIONS_MONTH', 'CUST_NUM_TRANSACTIONS_QUARTER',\n","       'CUST_NUM_TRANSACTIONS_SEMESTER', 'CUST_NUM_TRANSACTIONS_YEAR',\n","       'CUST_TOTAL_QTY_BOUGHT_MONTH', 'CUST_TOTAL_QTY_BOUGHT_QUARTER',\n","       'CUST_TOTAL_QTY_BOUGHT_SEMESTER', 'CUST_TOTAL_QTY_BOUGHT_YEAR',\n","       'CUST_NUM_UNIQUE_SUBCAT_MONTH', 'CUST_NUM_UNIQUE_SUBCAT_QUARTER',\n","       'CUST_NUM_UNIQUE_SUBCAT_SEMESTER', 'CUST_NUM_UNIQUE_SUBCAT_YEAR',\n","       'CUST_AVG_DAYS_SINCE_PRIOR_TRANSACTION_MONTH',\n","       'CUST_AVG_DAYS_SINCE_PRIOR_TRANSACTION_QUARTER',\n","       'CUST_AVG_DAYS_SINCE_PRIOR_TRANSACTION_SEMESTER',\n","       'CUST_AVG_DAYS_SINCE_PRIOR_TRANSACTION_YEAR','CUST_AVG_BASKET_SIZE_MONTH', 'CUST_AVG_BASKET_SIZE_QUARTER',\n","       'CUST_AVG_BASKET_SIZE_SEMESTER', 'CUST_AVG_BASKET_SIZE_YEAR',\n","       'SUBCAT_NUM_TRANSACTIONS_MONTH', 'SUBCAT_NUM_TRANSACTIONS_QUARTER',\n","       'SUBCAT_NUM_TRANSACTIONS_SEMESTER', 'SUBCAT_NUM_TRANSACTIONS_YEAR',\n","       'SUBCAT_TOTAL_QTY_BOUGHT_MONTH', 'SUBCAT_TOTAL_QTY_BOUGHT_QUARTER',\n","       'SUBCAT_TOTAL_QTY_BOUGHT_SEMESTER', 'SUBCAT_TOTAL_QTY_BOUGHT_YEAR',\n","       'SUBCAT_NUM_UNIQUE_CUST_MONTH', 'SUBCAT_NUM_UNIQUE_CUST_QUARTER',\n","       'SUBCAT_NUM_UNIQUE_CUST_SEMESTER', 'SUBCAT_NUM_UNIQUE_CUST_YEAR',\n","       'CUSTSUBCAT_NUM_TRANSACTIONS_MONTH',\n","       'CUSTSUBCAT_NUM_TRANSACTIONS_QUARTER',\n","       'CUSTSUBCAT_NUM_TRANSACTIONS_SEMESTER',\n","       'CUSTSUBCAT_NUM_TRANSACTIONS_YEAR', 'CUSTSUBCAT_TOTAL_QTY_BOUGHT_MONTH',\n","       'CUSTSUBCAT_TOTAL_QTY_BOUGHT_QUARTER',\n","       'CUSTSUBCAT_TOTAL_QTY_BOUGHT_SEMESTER',\n","       'CUSTSUBCAT_TOTAL_QTY_BOUGHT_YEAR',\n","       'CUSTSUBCAT_AVG_DAYS_SINCE_PRIOR_TRANSACTION_MONTH',\n","       'CUSTSUBCAT_AVG_DAYS_SINCE_PRIOR_TRANSACTION_QUARTER',\n","       'CUSTSUBCAT_AVG_DAYS_SINCE_PRIOR_TRANSACTION_SEMESTER',\n","       'CUSTSUBCAT_AVG_DAYS_SINCE_PRIOR_TRANSACTION_YEAR']\n","\n","scaler = MinMaxScaler()\n","df_500k[numerical_columns] = scaler.fit_transform(df_500k[numerical_columns])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dULFfLrTQjqM"},"source":["## Feature selection: Filter methods - No need to run"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1683754102920,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"TqKan9Es5eia"},"outputs":[],"source":["def our_heatmap(df_values, threshold=1.1):\n","\n","    # Define the colors\n","    colors = ['#84161a', '#fcf2f2']\n","\n","    # Create a list of relative positions for each color\n","    positions = [0, 1]\n","\n","    # Create the custom colormap\n","    cmap = mcolors.LinearSegmentedColormap.from_list(\"\", list(zip(positions, colors)))\n","\n","    # keep only the correlations above the threshold\n","    df_values = df_values[df_values < threshold]\n","\n","    # Plot the heatmap with the custom colormap\n","    fig, ax = plt.subplots(figsize=(20, 15))\n","    sns.heatmap(df_values, annot=True, cmap=cmap, vmin=0, vmax=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urAVb8JUJhNR"},"outputs":[],"source":["# calculate correlation matrix\n","corr = df_200k[numerical_columns].corr()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":5705,"status":"ok","timestamp":1683468422549,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"EUE-h_CBJhXH","outputId":"56ee2832-9fe8-40eb-cd12-82bdcf940bf6"},"outputs":[],"source":["# set the threshold\n","threshold = 0.7\n","\n","# keep only the correlations above the threshold\n","corr = corr[corr > threshold]\n","\n","# Create the figure and the axes objects\n","fig, ax = plt.subplots(figsize=(20, 15))\n","sns.heatmap(corr,annot = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":308},"executionInfo":{"elapsed":985032,"status":"ok","timestamp":1683542980865,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"gPPTmho9MHrj","outputId":"20b9db15-7ece-4116-d566-db67cc1f0590"},"outputs":[],"source":["# Bootstrap sampling to identify associations/correlations between categorical variables\n","\n","categorical_columns = ['SUBCAT_CD_EXT','MONTH','QUARTER','SEMESTER','YEAR','GENDER','SEG_LIFESTYLE_CD','SEG_LIFESTAGE_CD','CAT_CD_EXT', 'FAMILY_MEMBERS']\n","\n","df_200k = df_200k[categorical_columns]\n","\n","# Create an empty matrix to store the p-values\n","pvals = np.zeros((len(df_200k.columns), len(df_200k.columns)))\n","\n","n_bootstraps = 1000\n","\n","pvals_ind = [None]*1000\n","\n","# Loop through all pairs of variables and calculate the p-value\n","for i, var1 in enumerate(df_200k.columns):\n","    for j, var2 in enumerate(df_200k.columns):\n","        if i == j:\n","            continue\n","        else:\n","            for k in range(0, n_bootstraps):\n","                sample = df_200k.sample(200, replace=True)\n","                cont_table = pd.crosstab(sample[var1], sample[var2])\n","                chi2, pval, dof, expected = chi2_contingency(cont_table)\n","                pvals_ind[k] = pval\n","        \n","            pvals[i, j] = np.mean(pvals_ind)\n","\n","# Convert the matrix to a data frame and print the results\n","pvals_df = pd.DataFrame(pvals, columns=df_200k.columns, index=df_200k.columns)\n","\n","pvals_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":3207,"status":"ok","timestamp":1683543256674,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"V2URMWeN8hW5","outputId":"13b1952d-a1f8-4f1d-99c5-adb96856f4f0"},"outputs":[],"source":["our_heatmap(pvals_df)\n","our_heatmap(pvals_df, 0.05)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## One-hot Encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1683754102921,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"9-KisaGe2XFo"},"outputs":[],"source":["df_200k = df_500k.drop(columns=['QUARTER','SEMESTER','CAT_CD_EXT','SEG_LIFESTYLE_CD', 'YEAR', 'MONTH', 'SUBCAT_CD_EXT','REG_AVG_DAYS_SINCE_PRIOR_TRANSACTION_MONTH'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1683754102924,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"yxtfCt7t3FiY"},"outputs":[],"source":["# One-hot encoding categorical variables\n","df_200k = pd.get_dummies(df_200k, columns=['GENDER','SEG_LIFESTAGE_CD', 'FAMILY_MEMBERS'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_200k.value_counts('fulldate').sort_index()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qSmWT8Ts5VjK"},"source":["## Outlier detection (unsupervised learning) - No need to run"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1683754102926,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"Pz5_ld61RMRk"},"outputs":[],"source":["def plot_3d_clusters(df_clst, z_rot):\n","    # Compute the first three principal components of the data\n","    pca = PCA(n_components=3)\n","    principal_components = pca.fit_transform(df_clst)\n","\n","    # Create a scatter plot of the first three principal components, colored by cluster\n","    cluster_colors = np.array(['#de1c26', 'yellow', 'orange', 'red', 'purple', 'black', 'gray', 'magenta', '#84161a'])\n","    sns.set_style(\"ticks\", {\"axes.facecolor\": \"#ffffff\"})\n","    fig = plt.figure(figsize=(20, 15))\n","    ax = fig.add_subplot(111, projection='3d')\n","    ax.scatter(principal_components[:,0], principal_components[:,1], principal_components[:,2], c=cluster_colors[df_clst['cluster'].values % len(cluster_colors)], alpha=0.8)\n","    ax.scatter(principal_components[outliers_mask, 0], principal_components[outliers_mask, 1], principal_components[outliers_mask, 2], c='#84161a', marker='X', s=100, alpha=1.0)\n","    ax.view_init(elev=10, azim=z_rot)\n","    \n","    ax.set_xlabel('PC 1')\n","    ax.set_ylabel('PC 2')\n","    ax.set_zlabel('PC 3')\n","\n","    ax.w_xaxis.set_pane_color('#fcf2f2')  # Change x-axis color\n","    ax.w_yaxis.set_pane_color('#fcf2f2')  # Change y-axis color\n","    ax.w_zaxis.set_pane_color('#fcf2f2')  # Change z-axis color\n","\n","    \n","    ax.w_xaxis.set_ticklabels([])\n","    ax.w_yaxis.set_ticklabels([])\n","    ax.w_zaxis.set_ticklabels([])\n","    #ax.grid(False)\n","\n","    plt.show()\n","\n","\n","def plot_2d_clusters(df_clst):\n","    # Compute the first two principal components of the data\n","    pca = PCA(n_components=2)\n","    principal_components = pca.fit_transform(df_clst)\n","\n","    # Create a scatter plot of the first three principal components, colored by cluster\n","    cluster_colors = np.array(['#de1c26', 'yellow', 'orange', 'red', 'purple', 'black', 'gray', 'magenta', '#84161a'])\n","    sns.set_style(\"ticks\", {\"axes.facecolor\": \"#ffffff\"})\n","    fig = plt.figure(figsize=(15, 10))\n","    ax = fig.add_subplot(111)\n","    ax.scatter(principal_components[:,0], principal_components[:,1], c=cluster_colors[df_clst['cluster'].values % len(cluster_colors)], alpha=0.8)\n","    ax.scatter(principal_components[outliers_mask, 0], principal_components[outliers_mask, 1], c='#84161a', marker='X', s=100, alpha=1.0)\n","\n","    ax.set_facecolor('#fcf2f2')\n","    ax.set_xticklabels([])\n","    ax.set_yticklabels([])\n","    \n","    plt.xlabel('PC 1')\n","    plt.ylabel('PC 2')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":43336,"status":"ok","timestamp":1683754146248,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"EEkLZxGA5V1T","outputId":"ed213bd9-b0f1-418a-8bac-ce01a1c6e7ff"},"outputs":[],"source":["# Extract the numerical columns\n","df_200k_out = df_200k[numerical_columns]\n","\n","batch_size = 50000\n","\n","while len(df_200k_out) > batch_size:\n","    df_50k = df_200k_out.sample(batch_size, replace=False)\n","    # Perform DBSCAN clustering and obtain cluster labels\n","    dbscan = DBSCAN(eps=0.6, min_samples=800)\n","    labels = dbscan.fit_predict(df_50k)\n","\n","    # Identify the outliers\n","    outliers_mask = labels == -1\n","    outliers = df_50k[outliers_mask]\n","    df_200k_out = df_200k_out.drop(index=df_50k.index)\n","    df_200k = df_200k.drop(index=outliers.index)\n","\n","    print(\"Number of outliers:\", len(outliers))\n","\n","    # Add cluster labels to the original dataframe\n","    df_50k['cluster'] = labels\n","\n","\n","# Perform DBSCAN clustering and obtain cluster labels\n","dbscan = DBSCAN(eps=0.6, min_samples=800)\n","labels = dbscan.fit_predict(df_200k_out)\n","\n","# Identify the outliers\n","outliers_mask = labels == -1\n","outliers = df_200k_out[outliers_mask]\n","df_200k = df_200k.drop(index=outliers.index)\n","\n","print(\"Number of outliers:\", len(outliers))\n","\n","# Add cluster labels to the original dataframe\n","df_200k_out['cluster'] = labels\n","\n","plot_2d_clusters(df_200k_out)\n","\n","plot_3d_clusters(df_200k_out, 0)\n","plot_3d_clusters(df_200k_out, 90)\n","plot_3d_clusters(df_200k_out, 180)\n","plot_3d_clusters(df_200k_out, 270)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eIPUlj_0TaRC"},"source":["## Machine Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1683754146249,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"zRzAeb9STadN"},"outputs":[],"source":["def get_results(model, gs ,name, data, true_labels, target_names = ['No buy', 'Buy'], results=None, reasume=False):\n","    global param\n","    \n","    if hasattr(model, 'layers'):\n","        param = wtp_dnn_model.history.params\n","        best = np.mean(history.history['val_accuracy'])\n","        # predicted_labels_test = np.argmax(model.predict(data) , axis=-1)\n","        # print(f'This is Predicted_Labels_test in the first if :{predicted_labels_test}')\n","        predicted_labels = (model.predict_proba(data)[:,1] >= 0.5).astype(int)\n","        print(f'\\n\\nThis is Predicted_Labels in the first if:{predicted_labels}\\n\\n')\n","        im_model = InMemoryModel(model.predict, examples=data, target_names=target_names)\n","\n","    else:\n","        \n","        param = gs.best_params_\n","        best = gs.best_score_\n","        # predicted_labels_test = model.predict(data).ravel()\n","        # print(f'This is Predicted_Labels_test - else:{predicted_labels_test}')\n","        predicted_labels = (model.predict_proba(data)[:,1] >= 0.5).astype(int)\n","        print(f'\\n\\nThis is Predicted_Labels - else :{predicted_labels}\\n\\n')\n","\n","        if hasattr(model, 'predict_proba'):\n","            #predicted_probs = model.predict_proba(data)[:, 1]\n","            #print(f'This is the predict_proba results but the ones from the get_results function: {predicted_probs}')\n","            print('This model has a predict_proba method')\n","        elif hasattr(model, 'decision_function'):\n","            im_model = InMemoryModel(model.decision_function, examples=data, target_names=target_names)\n","        else: \n","            print('Cannot use InMemoryModel as predict_proba is not available')\n","           \n","        \n","    print('Mean Best Accuracy: {:2.2%}'.format(best))\n","    print('-'*60)\n","    print('Best Parameters:')\n","    print(param)\n","    print('-'*60)\n","    \n","    #y_pred = model.predict(data).ravel()\n","\n","    #y_predict_class = [1 if prob > 0.4 else 0 for prob in y_predict_prob_class_1]\n","\n","    # y_pred = (model.predict_proba(data)[:,1] >= 0.7).astype(bool)\n","\n","    y_pred = (model.predict_proba(data)[:,1] >= 0.5).astype(int).ravel()\n","    #y_pred = int(y_pred)\n","\n","    display_model_performance_metrics(true_labels, predicted_labels = predicted_labels, target_names = target_names)\n","    if len(target_names)==2:\n","        ras = roc_auc_score(y_true=true_labels, y_score=y_pred)\n","    else:\n","        roc_auc_multiclass, ras = roc_auc_score_multiclass(y_true=true_labels, y_score=y_pred, target_names=target_names)\n","        print('\\nROC AUC Score by Classes:\\n',roc_auc_multiclass)\n","        print('-'*60)\n","\n","    print('\\n\\n              ROC AUC Score: {:2.2%}'.format(ras))\n","    prob, score_roc, roc_auc = plot_model_roc_curve(model, data, true_labels, label_encoder=None, class_names=target_names)\n","    \n","    #interpreter = Interpretation(data, feature_names=cols)\n","    #plots = interpreter.feature_importance.plot_feature_importance(im_model, progressbar=False, n_jobs=1, ascending=True)\n","    \n","    r1 = pd.DataFrame([(prob, best, np.round(accuracy_score(true_labels, predicted_labels), 4), \n","                         ras, roc_auc)], index = [name],\n","                         columns = ['Prob', 'CV Accuracy', 'Accuracy', 'ROC AUC Score', 'ROC Area'])\n","    if reasume:\n","        results = r1\n","    elif (name in results.index):        \n","        results.loc[[name], :] = r1\n","    else: \n","        results = results.append(r1)\n","        \n","    return y_pred, results \n","\n","\n","def roc_auc_score_multiclass(y_true, y_score, target_names, average = \"macro\"):\n","\n","  #creating a set of all the unique classes using the actual class list\n","  unique_class = set(y_true)\n","  roc_auc_dict = {}\n","  mean_roc_auc = 0\n","  for per_class in unique_class:\n","    #creating a list of all the classes except the current class \n","    other_class = [x for x in unique_class if x != per_class]\n","\n","    #marking the current class as 1 and all other classes as 0\n","    new_y_true = [0 if x in other_class else 1 for x in y_true]\n","    new_y_score = [0 if x in other_class else 1 for x in y_score]\n","    num_new_y_true = sum(new_y_true)\n","\n","    #using the sklearn metrics method to calculate the roc_auc_score\n","    roc_auc = roc_auc_score(new_y_true, new_y_score, average = average)\n","    roc_auc_dict[target_names[per_class]] = np.round(roc_auc, 4)\n","    mean_roc_auc += num_new_y_true * np.round(roc_auc, 4)\n","    \n","  mean_roc_auc = mean_roc_auc/len(y_true)  \n","  return roc_auc_dict, mean_roc_auc\n","\n","def get_metrics(true_labels, predicted_labels):\n","    global accuracy\n","    global precision\n","    global recall\n","    global f1\n","    accuracy = metrics.accuracy_score(true_labels, predicted_labels)\n","    precision = metrics.precision_score(true_labels, predicted_labels)\n","    recall = metrics.recall_score(true_labels, predicted_labels)\n","    f1 = metrics.f1_score(true_labels, predicted_labels)\n","    \n","    print('Accuracy:  {:2.2%} '.format(accuracy))\n","    print('Precision: {:2.2%} '.format(precision))\n","    print('Recall:    {:2.2%} '.format(recall))\n","    print('F1 Score:  {:2.2%} '.format(f1))\n","    # #append results to arrays\n","    # np.append(accuracy_array, metrics.accuracy_score(true_labels, predicted_labels))\n","    # np.append(precision_array, metrics.precision_score(true_labels, predicted_labels, average='weighted'))\n","    # np.append(recall_array, metrics.recall_score(true_labels, predicted_labels, average='weighted'))\n","    # np.append(f1_array, metrics.f1_score(true_labels, predicted_labels, average='weighted'))\n","    \n","                        \n","\n","def train_predict_model(classifier,  train_features, train_labels,  test_features, test_labels):\n","    # build model    \n","    classifier.fit(train_features, train_labels)\n","    # predict using model\n","    predictions = classifier.predict(test_features) \n","    return predictions    \n","\n","\n","def display_confusion_matrix(true_labels, predicted_labels, target_names):\n","    \n","    total_classes = len(target_names)\n","    level_labels = [total_classes*[0], list(range(total_classes))]\n","\n","    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels)\n","    cm_frame = pd.DataFrame(data=cm, \n","                            columns=pd.MultiIndex(levels=[['Predicted:'], target_names], codes=level_labels), \n","                            index=pd.MultiIndex(levels=[['Actual:'], target_names], codes=level_labels)) \n","    print(cm_frame) \n","    \n","def display_classification_report(true_labels, predicted_labels, target_names):\n","\n","    report = metrics.classification_report(y_true=true_labels, y_pred=predicted_labels, target_names=target_names) \n","    print(report)\n","    \n","def display_model_performance_metrics(true_labels, predicted_labels, target_names):\n","    print('Model Performance metrics:')\n","    print('-'*30)\n","    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n","    print('\\nModel Classification report:')\n","    print('-'*30)\n","    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n","    print('\\nPrediction Confusion Matrix:')\n","    print('-'*30)\n","    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, target_names=target_names)\n","\n","def plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n","    \n","    ## Compute ROC curve and ROC area for each class\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    if hasattr(clf, 'classes_'):\n","        class_labels = clf.classes_\n","    elif label_encoder:\n","        class_labels = label_encoder.classes_\n","    elif class_names:\n","        class_labels = class_names\n","    else:\n","        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n","    n_classes = len(class_labels)\n","   \n","    if n_classes == 2:\n","        if hasattr(clf, 'predict_proba'):\n","            prb = clf.predict_proba(features)\n","            if prb.shape[1] > 1:\n","                y_score = prb[:, prb.shape[1]-1] \n","            else:\n","                y_score = clf.predict(features).ravel()\n","            prob = True\n","        elif hasattr(clf, 'decision_function'):\n","            y_score = clf.decision_function(features)\n","            prob = False\n","        else:\n","            print(\"\\n\")\n","            #raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n","        \n","        fpr, tpr, _ = roc_curve(true_labels, y_score)      \n","        roc_auc = auc(fpr, tpr)\n","\n","        plt.plot(fpr, tpr, label='ROC curve (area = {0:3.2%})'.format(roc_auc), linewidth=2.5)\n","        \n","    elif n_classes > 2:\n","        if  hasattr(clf, 'clfs_'):\n","            y_labels = label_binarize(true_labels, classes=list(range(len(class_labels))))\n","        else:\n","            y_labels = label_binarize(true_labels, classes=class_labels)\n","        if hasattr(clf, 'predict_proba'):\n","            y_score = clf.predict_proba(features)\n","            prob = True\n","        elif hasattr(clf, 'decision_function'):\n","            y_score = clf.decision_function(features)\n","            prob = False\n","        else:\n","            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n","            \n","        for i in range(n_classes):\n","            fpr[i], tpr[i], _ = roc_curve(y_labels[:, i], y_score[:, i])\n","            roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","        ## Compute micro-average ROC curve and ROC area\n","        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_labels.ravel(), y_score.ravel())\n","        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","        ## Compute macro-average ROC curve and ROC area\n","        # First aggregate all false positive rates\n","        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n","        # Then interpolate all ROC curves at this points\n","        mean_tpr = np.zeros_like(all_fpr)\n","        for i in range(n_classes):\n","            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n","        # Finally average it and compute AUC\n","        mean_tpr /= n_classes\n","        fpr[\"macro\"] = all_fpr\n","        tpr[\"macro\"] = mean_tpr\n","        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n","\n","        ## Plot ROC curves\n","        plt.figure(figsize=(12, 6))\n","        plt.plot(fpr[\"micro\"], tpr[\"micro\"], label='micro-average ROC curve (area = {0:2.2%})'\n","                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n","\n","        plt.plot(fpr[\"macro\"], tpr[\"macro\"], label='macro-average ROC curve (area = {0:2.2%})'\n","                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n","  \n","        for i, label in enumerate(class_names):\n","            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:2.2%})'\n","                                           ''.format(label, roc_auc[i]), linewidth=2, linestyle=':')\n","            \n","        roc_auc = roc_auc[\"macro\"]   \n","    else:\n","        raise ValueError('Number of classes should be atleast 2 or more')\n","        \n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([-0.01, 1.0])\n","    plt.ylim([0.0, 1.01])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\")\n","\n","    \n","    return prob, y_score, roc_auc"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1683754146250,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"crYEAm3FTafT"},"outputs":[],"source":["class select_fetaures(object):  # BaseEstimator, TransformerMixin,\n","     def __init__(self, select_cols):\n","         self.select_cols_ = select_cols\n","\n","     def fit(self, X, Y):\n","         pass\n","\n","     def transform(self, X):\n","         return X.loc[:, self.select_cols_]\n","\n","     def fit_transform(self, X, Y):\n","         self.fit(X, Y)\n","         df = self.transform(X)\n","         return df\n","\n","     def __getitem__(self, x):\n","         return self.X[x], self.Y[x]"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":251,"status":"ok","timestamp":1683761567686,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"Ve5RdgiUTaiA"},"outputs":[],"source":["def LR(X_train, y_train, X_train_tune, y_train_tune, X_test, y_test):\n","        \n","    clf = Pipeline([\n","                # ('pca', PCA()),\n","                ('clf', LogisticRegression())])  \n","\n","    param_grid = {}\n","\n","    # Use SequentialFeatureSelector for forward/backward selection\n","    sfs_backward = SequentialFeatureSelector(clf, n_features_to_select='auto', tol=0.00, direction='forward', scoring='precision', cv=None, n_jobs=-1)  # Does 5-Fold CV\n","\n","    # Fit the feature selector to the training data\n","    sfs_backward.fit(X_train_tune, y_train_tune)\n","\n","    # Get the selected features and transform the data\n","    X_train = sfs_backward.transform(X_train)\n","    X_test = sfs_backward.transform(X_test)\n","\n","    global features_selected\n","    features_selected = sfs_backward.feature_names_in_[sfs_backward.support_]\n","\n","    print('Number of selected features: {}'.format(sfs_backward.n_features_to_select_))\n","    print('Features selected: {}'.format(sfs_backward.feature_names_in_[sfs_backward.support_]))\n","        \n","    gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='precision', cv=2, verbose=1, n_jobs=-1)\n","    LR = Pipeline([('gs', gs)]) \n","        \n","    LR.fit(X_train,y_train)\n","    \n","    #print the number of features used in the model\n","    #print('Number of features used: {}'.format(LR.named_steps['gs'].best_estimator_.named_steps['clf'].coef_.shape[1]))\n","\n","    joblib.dump(LR, 'LR.joblib')\n","\n","\n","    global y_proba\n","    y_proba = LR.predict_proba(X_test)\n","\n","    \n","\n","    y_pred, results = get_results(LR, gs ,'Logistic Regression', X_test, y_test, reasume=True)\n","    print(results)\n","    print(f'This is the predict_proba results {y_proba}')\n","\n","    return y_pred\n","        \n","\n","def RF(X_train, y_train, X_train_tune, y_train_tune, X_test, y_test):\n","\n","    clf = Pipeline([\n","                # ('pca', PCA()),\n","                ('clf', RandomForestClassifier())])  \n","\n","    param_grid = {'clf__criterion': ['gini']  # , 'entropy', 'log_loss'\n","                  ,'clf__n_estimators':  [500]       \n","                  ,'clf__min_samples_split': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n","                  ,'clf__max_depth': [10, 50, 100, 150, 200]\n","                  ,'clf__min_samples_leaf': [20, 40, 60, 80, 100]}\n","    \n","    # Use SequentialFeatureSelector for forward/backward selection\n","    sfs_backward = SequentialFeatureSelector(clf, n_features_to_select='auto', tol=0.00, direction='forward', scoring='recall', cv=None, n_jobs=-1)  # Does 5-Fold CV\n","\n","    # Fit the feature selector to the training data\n","    sfs_backward.fit(X_train_tune, y_train_tune)\n","\n","    global features_selected\n","    features_selected = sfs_backward.feature_names_in_[sfs_backward.support_]\n","\n","    # Get the selected features and transform the data\n","    X_train = sfs_backward.transform(X_train)\n","    X_test = sfs_backward.transform(X_test)\n","\n","    print('Number of selected features: {}'.format(sfs_backward.n_features_to_select_))\n","    print('Features selected: {}'.format(sfs_backward.feature_names_in_[sfs_backward.support_]))\n","\n","    gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='recall', cv=2, verbose=1, n_jobs=-1)\n","    RF = Pipeline([('gs', gs)]) \n","        \n","    RF.fit(X_train,y_train)\n","\n","    joblib.dump(RF, 'RF.joblib')\n","\n","\n","    global y_proba\n","    y_proba = RF.predict_proba(X_test)\n","\n","    y_pred, results = get_results(RF, gs ,'Random Forest', X_test, y_test, reasume=True)\n","    print(results)\n","    print(f'This is the predict_proba results {y_proba}')\n","\n","    return y_pred\n","\n","\n","def SVM(X_train, y_train, X_train_tuning, y_train_tuning, X_test, y_test):\n","\n","    clf = Pipeline([\n","                # ('pca', PCA()),\n","                ('clf', svm.SVC())])  \n","\n","    param_grid = {'clf__C': [0.05, 0.1, 0.15, 0.2]  # 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\n","                  ,'clf__kernel' : ['rbf']}   #, 'linear', , 'poly', 'linear', 'sigmoid', 'precomputed'\n","\n","    # Use SequentialFeatureSelector for backward selection\n","    sfs_backward = SequentialFeatureSelector(clf, n_features_to_select='auto', tol=0.05, direction='forward', scoring='f1', cv=None, n_jobs=-1)  # Does 5-Fold CV\n","\n","    # Fit the feature selector to the training data\n","    sfs_backward.fit(X_train_tuning, y_train_tuning)\n","\n","    # Get the selected features and transform the data\n","    X_train = sfs_backward.transform(X_train)\n","    X_test = sfs_backward.transform(X_test)\n","\n","    print('Number of selected features: {}'.format(sfs_backward.n_features_to_select_))\n","    print('Features selected: {}'.format(sfs_backward.feature_names_in_[sfs_backward.support_]))\n","        \n","    gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=1, n_jobs=-1)\n","    SVM = Pipeline([('gs', gs)]) \n","        \n","    SVM.fit(X_train,y_train)\n","\n","    results = get_results(SVM, gs ,'Support Vector Machines', X_test, y_test, reasume=True)\n","    print(results)\n","    \n","\n","def ANN(X_train, y_train, X_train_tune, y_train_tune, X_test, y_test):\n","    \n","    clf = Pipeline([\n","            # ('pca', PCA()),\n","            ('clf', MLPClassifier())])  \n","\n","    param_grid = {'clf__hidden_layer_sizes': [(64,), (128,), (256,)],\n","                  'clf__activation': ['relu'],\n","                  'clf__solver': ['adam'],\n","                  'clf__early_stopping': [True],  # creates a stratified validation set (10% of training data)\n","                  'clf__validation_fraction': [0.2], \n","                  'clf__n_iter_no_change': [10],\n","                  'clf__alpha': [0.0001, 0.001],\n","                  'clf__learning_rate': ['constant','invscaling','adaptive'],\n","                  'clf__tol': [0.0001, 0.001],\n","                  'clf__learning_rate_init': [0.0001, 0.001],\n","                  'clf__max_iter': [1000]}\n","\n","    # Use SequentialFeatureSelector for forward/backward selection\n","    sfs_backward = SequentialFeatureSelector(clf, n_features_to_select='auto', tol=0.00, direction='forward', scoring='f1', cv=None, n_jobs=-1)  # Does 5-Fold CV\n","\n","    # Fit the feature selector to the training data\n","    sfs_backward.fit(X_train_tune, y_train_tune)\n","\n","    # Get the selected features and transform the data\n","    X_train = sfs_backward.transform(X_train)\n","    X_test = sfs_backward.transform(X_test)\n","\n","    print('Number of selected features: {}'.format(sfs_backward.n_features_to_select_))\n","    print('Features selected: {}'.format(sfs_backward.feature_names_in_[sfs_backward.support_]))\n","\n","    gs = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=2, verbose=1, n_jobs=-1)\n","    ANN = Pipeline([('gs', gs)]) \n","        \n","    ANN.fit(X_train,y_train)\n","\n","    results = get_results(ANN, gs ,'Neural Network', X_test, y_test, reasume=True)\n","    print(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":211,"status":"ok","timestamp":1683754243983,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"vD74h9UzTakT"},"outputs":[],"source":["def cv_12months(df_ML, model, resampling_tecnique):\n","    timestamps = df_ML['fulldate'].sort_values().unique()\n","    #display(timestamps.size)\n","\n","    # find the minimum timestamp\n","    min_timestamp = timestamps[0]\n","\n","    #store each of the metrics in an array for each one\n","    accuracy_array = np.array([])\n","    precision_array = np.array([])\n","    recall_array = np.array([])\n","    f1_array = np.array([])\n","\n","    \n","    probability_array = np.array([])\n","\n","    # full_xtest_df = []\n","\n","    df_total = pd.DataFrame()\n","\n","\n","    full_test_count = 0\n","    \n","\n","    \n","    # iterate over the timestamps and check if the time difference is less than 12 months\n","    for ts in timestamps[:-3]:\n","        \n","        if ts + 3 < timestamps.max():\n","            print(f'\\n\\nStart Iteration {ts}\\n\\n')\n","            if (ts - min_timestamp).n < 2: # 11\n","               \n","                print(f'Treino feito com os meses: {ts} até {ts+2}') # 11   \n","                train = df_ML[df_ML['fulldate'].isin([ts, ts+1, ts+2])]  # ts+3, ts+4, ts+5, ts+6, ts+7, ts+8, ts+9, ts+10, ts+11\n","                train_tune = train.groupby('fulldate').sample(frac=0.3, random_state=101)\n","                X_train_cv = train.drop(['TARGET','fulldate'], axis=1)\n","                y_train_cv = train['TARGET'].astype(int)\n","                X_train_tune = train_tune.drop(['TARGET','fulldate'], axis=1)\n","                y_train_tune = train_tune['TARGET'].astype(int)                \n","                \n","                X_train_cv, y_train_cv = resampling_tecnique(X_train_cv, y_train_cv)\n","                X_train_tune, y_train_tune = random_undersampling(X_train_tune, y_train_tune)\n","\n","                print('X_train_cv shape: ', X_train_cv.shape)\n","                print('X_train_tune shape: ', X_train_tune.shape)\n","            else:\n","               \n","                print(f'Treino feito com os meses: {ts} até {ts+2}') # 11\n","            \n","                train = df_ML[df_ML['fulldate'].isin([ts, ts+1, ts+2])]  # ts+3, ts+4, ts+5, ts+6, ts+7, ts+8, ts+9, ts+10, ts+11\n","                train_tune = train.groupby('fulldate').sample(frac=0.3, random_state=101)\n","                X_train_cv = train.drop(['TARGET','fulldate'], axis=1)\n","                y_train_cv = train['TARGET'].astype(int)\n","                X_train_tune = train_tune.drop(['TARGET','fulldate'], axis=1)\n","                y_train_tune = train_tune['TARGET'].astype(int) \n","\n","                X_train_cv, y_train_cv = resampling_tecnique(X_train_cv, y_train_cv)\n","                X_train_tune, y_train_tune = random_undersampling(X_train_tune, y_train_tune)\n","                \n","                print('X_train_cv shape: ', X_train_cv.shape)\n","                print('X_train_tune shape: ', X_train_tune.shape)\n","            \n","            print(f'Testado com mês: {ts+3}')\n","            test = df_ML[df_ML['fulldate'] == ts+3] # 12\n","            # print(f'Teste Fulldate: {test[\"fulldate\"].unique()}')\n","            # print(f'Ts: {ts+3}')\n","            X_test = test.drop(['TARGET', 'fulldate'], axis=1)\n","\n","            X_test_pandas = pd.DataFrame(X_test, columns=X_test.columns)\n","\n","            df_total = df_total.append(X_test_pandas)\n","            \n","\n","            # full_xtest_df = pd.concat(X_test)\n","\n","            full_test_count += X_test.shape[0]\n","\n","\n","            y_test = test['TARGET'].astype(int)\n","            print(f'X_test shape do mês {ts +3}: ', X_test.shape)\n","            #print(f'Teste feito com o mês: {ts+3}')  # 12\n","\n","            \n","            #print y_train_cv data type\n","            \n","            y_pred = model(X_train_cv, y_train_cv, X_train_tune, y_train_tune, X_test, y_test)\n","            # print(f'Treino feito com os meses: {ts} até {ts+2}') # 11\n","            # print(f'Teste feito com o mês: {ts+3}')  # 12\n","            # print('X_train_cv shape: ', X_train_cv.shape)\n","            # print('X_train_tune shape: ', X_train_tune.shape)\n","            # print(f'Y_pred shape: {y_pred.shape}')\n","            #append accuracy variable to the accuracy array\n","            accuracy_array = np.append(accuracy_array, accuracy)\n","            #append precision variable to the precision array\n","            precision_array = np.append(precision_array, precision)\n","            #append recall variable to the recall array\n","            recall_array = np.append(recall_array, recall)\n","            #append f1 variable to the f1 array\n","            f1_array = np.append(f1_array, f1)\n","            \n","            # Get the model exported in the last iteration of the model\n","            model.features = features_selected\n","            model.paramethers = param\n","            model_exported = model\n","\n","            \n","\n","\n","            probability_array = np.append(probability_array, y_proba)\n","            print('\\nEnd Iteration\\n')\n","            #pint barrier or * to separate the iterations\n","            print('******************************************************************************************')\n","        else: \n","            print(f'\\nFinish the for loop\\n')\n","            print(f'Accuracy Mean of all iterations : {np.mean(accuracy_array)}')\n","            print(f'Precision Mean of all iterations : {np.mean(precision_array)}')\n","            print(f'Recall Mean of all iterations: {np.mean(recall_array)}')\n","            print(f'F1 mean of all iterations: {np.mean(f1_array)}')\n","            return X_train_cv, y_train_cv,X_test, y_test , probability_array, df_total, y_pred, features_selected, param, model_exported\n","\n","    #print(f'\\n\\nNumber of test samples: {full_test_count}')\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#save model exported using joblib\n","def save_model(model, model_name):\n","    joblib.dump(model, model_name)\n","    print(f'\\nModel {model_name} saved successfully!')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":265,"status":"ok","timestamp":1683754247606,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"kc-gKvzCTamG"},"outputs":[],"source":["def random_oversampling(x_train,y_train):\n","    ros = RandomOverSampler(random_state=101)\n","    X_train, y_train = ros.fit_resample(x_train, y_train)\n","    \n","    return X_train, y_train\n","\n","def roSMOTE(x_train,y_train):\n","    os = SMOTE(random_state=101)\n","    X_train, y_train = os.fit_resample(x_train, y_train)\n","    \n","    return X_train, y_train\n","\n","def random_undersampling(x_train,y_train):\n","    rus = RandomUnderSampler(random_state=101)\n","    X_train, y_train = rus.fit_resample(x_train, y_train)\n","    \n","    return X_train, y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":616271,"status":"ok","timestamp":1683762644572,"user":{"displayName":"Luís Henriques","userId":"06170356873631694942"},"user_tz":-60},"id":"gAbCKTxnTap9","outputId":"a7099c1e-85e5-4a2b-9676-d317fac7e0fa"},"outputs":[],"source":["X_train_cv, y_train_cv,X_test, y_test , y_proba, df_total, y_pred, features_selected, param, model = cv_12months(df_200k, LR, random_undersampling)   # Taking too long... What about removing all subcategory columns with all values 0?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features_selected"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Recommendation System - Top 20 Subcategories for each Customer in the month of November\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Normal recommendation with model ran above"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#aggregate the results of y_proba in tuples of 2\n","y_proba = y_proba.reshape(int(y_proba.shape[0]/2), 2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_total['PROBABILITIES'] = y_proba.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def recommend_fixed():\n","    df_full = df.groupby('fulldate').sample(frac=0.2, random_state=101)\n","    df_nov = df_full[df_full['fulldate']=='2022-11']\n","    #reset index of df_200k_oct\n","    df_nov = df_nov.reset_index(drop=True)\n","    y_proba_last = y_proba[-y_pred.shape[0]:]\n","    y_proba_df = pd.DataFrame(y_proba_last, columns=['NO BUY', 'BUY'])\n","    y_pred_df = pd.DataFrame(y_pred, columns=['Target'])\n","    joined_prob_pred = y_pred_df.join(y_proba_df)\n","    df_oct_joined = df_nov.join(joined_prob_pred)\n","    df_oct_joined = df_oct_joined[['CUSTOMER_ACCOUNT_NR_MASK', 'SUBCAT_CD_EXT', 'SEG_LIFESTYLE_CD',  'NO BUY', 'BUY', 'Target']]\n","    #select only the rows where Target = 1\n","    df_oct_joined = df_oct_joined[df_oct_joined['Target'] == 1]\n","    #group by customer account nr mask and sort by buy\n","    recs_grouped = df_oct_joined.groupby('CUSTOMER_ACCOUNT_NR_MASK').apply(lambda x: x.sort_values(by='BUY', ascending=False))\n","    #rename CUSTOMER_ACCOUNT_NR_MASK to CUSTOMER\n","    recs_grouped = recs_grouped.rename(columns={'CUSTOMER_ACCOUNT_NR_MASK': 'CUSTOMER'})\n","    top_subcategories = recs_grouped.groupby('CUSTOMER_ACCOUNT_NR_MASK').apply(lambda x: x.nlargest(20, 'BUY'))\n","    \n","\n","    return recs_grouped, top_subcategories"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["recs_grouped, top_subcategories = recommend_fixed()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["recs_grouped"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Recommendations using .joblib file\n","\n","Independentemente do mês escolhido, as recomendações vão ser geradas com base nas features da última iteração do modelo"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def recommend_joblib(month, joblib_file):\n","\n","    df_full_job = df.groupby('fulldate').sample(frac=0.2, random_state=101)\n","    df_nov_job = df_full_job[df_full_job['fulldate']==month]\n","\n","    \n","    test_job = df_200k[df_200k['fulldate'] == month] \n","    X_test_job = test_job.drop(['TARGET', 'fulldate'], axis=1)\n","\n","    X_test_pandas_job = pd.DataFrame(X_test_job, columns=X_test_job.columns)\n","    model_job = joblib.load(joblib_file)\n","    X_test_deploy_job = X_test_pandas_job[features_selected]\n","    y_proba_job = model_job.predict_proba(X_test_deploy_job)\n","    y_pred_job = model_job.predict(X_test_deploy_job)\n","    X_test_pandas_job['PROBABILITIES'] = y_proba_job.tolist()\n","   \n","    #reset index of df_200k_oct\n","    df_nov_job = df_nov_job.reset_index(drop=True)\n","    y_proba_last = y_proba_job[-y_pred_job.shape[0]:]\n","    y_proba_df = pd.DataFrame(y_proba_last, columns=['NO BUY', 'BUY'])\n","    y_pred_df = pd.DataFrame(y_pred_job, columns=['Target'])\n","    joined_prob_pred = y_pred_df.join(y_proba_df)\n","    df_oct_joined = df_nov_job.join(joined_prob_pred)\n","    df_oct_joined = df_oct_joined[['CUSTOMER_ACCOUNT_NR_MASK', 'SUBCAT_CD_EXT', 'SEG_LIFESTYLE_CD',  'NO BUY', 'BUY', 'Target']]\n","    #select only the rows where Target = 1\n","    df_oct_joined = df_oct_joined[df_oct_joined['Target'] == 1]\n","    #group by customer account nr mask and sort by buy\n","    recs_grouped = df_oct_joined.groupby('CUSTOMER_ACCOUNT_NR_MASK').apply(lambda x: x.sort_values(by='BUY', ascending=False))\n","    #rename CUSTOMER_ACCOUNT_NR_MASK to CUSTOMER\n","    recs_grouped_job = recs_grouped.rename(columns={'CUSTOMER_ACCOUNT_NR_MASK': 'CUSTOMER'})\n","    top20_subcategories_job = recs_grouped_job.groupby('CUSTOMER_ACCOUNT_NR_MASK').apply(lambda x: x.nlargest(20, 'BUY'))\n","    \n","\n","    return recs_grouped_job, top20_subcategories_job\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["recs_grouped_job, top20_subcategories_job = recommend_joblib('2022-11', 'LR.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["recs_grouped_job"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Calculation of frequencies for subcategories (recommendations and total dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#select only rows where TARGET is equal to 1\n","df_only_1s = df_500k[df_500k['TARGET'] == 1]\n","#show most frequent SUBCAT_CD_EXT for each SEG_LIFESTYLE_CD\n","lifestyles_w_subcat = pd.DataFrame(df_only_1s.groupby('SEG_LIFESTYLE_CD')['SUBCAT_CD_EXT'].value_counts())\n","\n","#instead of using the absolute number, use the percentage of each SUBCAT_CD_EXT for each SEG_LIFESTYLE_CD\n","lifestyles_w_subcat['PERC_TOTAL'] = lifestyles_w_subcat.groupby(level=0).apply(lambda x: 100 * x / float(x.sum()))\n","\n","#rename the column SUBCAT_CD_EXT to FREQ_TOTAL\n","lifestyles_w_subcat.rename(columns={'SUBCAT_CD_EXT': 'FREQ_TOTAL'}, inplace=True)\n","\n","#make a copy of lifestyles_w_subcat\n","lifestyles_w_subcat_copy = lifestyles_w_subcat.copy()\n","\n","#reset the index\n","lifestyles_w_subcat.reset_index(inplace=True)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["freq_customer = recs_grouped['CUSTOMER'].value_counts().idxmax()\n","\n","recs_for_freqc = recs_grouped[recs_grouped['CUSTOMER'] == freq_customer]\n","\n","#show the most frequent subcategories for each unique SEG_LIFESTYLE_CD\n","freq_subcats = pd.DataFrame(top_subcategories.groupby('SEG_LIFESTYLE_CD')['SUBCAT_CD_EXT'].value_counts())\n","pd.set_option('display.max_rows', None)\n","#rename SUBCAT_CD_EXT to FREQUENCY\n","freq_subcats = freq_subcats.rename(columns={'SUBCAT_CD_EXT': 'FREQUENCY'})\n","#create new column with percentage of each subcategory\n","freq_subcats['PERCENTAGE'] = freq_subcats.groupby(level=0).apply(lambda x: 100 * x / float(x.sum()))\n","\n","#reset index from teste\n","teste_reseted = freq_subcats.reset_index()\n","\n","#join teste_reseted with recs_for_freqc on SEG_LIFESTYLE_CD and SUBCAT_CD_EXT\n","joined_both = teste_reseted.merge(recs_for_freqc, on=['SEG_LIFESTYLE_CD', 'SUBCAT_CD_EXT'])\n","\n","#rename frequency to freq_recs and percentage to perc_recs\n","joined_both = joined_both.rename(columns={'FREQUENCY': 'FREQ_RECS', 'PERCENTAGE': 'PERC_RECS'})\n","\n","#join joined_both with lifestyles_w_subcat on SEG_LIFESTYLE_CD and SUBCAT_CD_EXT\n","joined_wo_dsc = joined_both.merge(lifestyles_w_subcat, on=['SEG_LIFESTYLE_CD', 'SUBCAT_CD_EXT'])\n","\n","ordenar_colunas = ['CUSTOMER', 'SEG_LIFESTYLE_CD', 'SUBCAT_CD_EXT', 'FREQ_RECS', 'FREQ_TOTAL', 'PERC_RECS', 'PERC_TOTAL'] \n","\n","joined_wo_dsc = joined_wo_dsc[ordenar_colunas]\n","\n","joined_wo_dsc\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#order by perc_recs\n","joined_wo_dsc.sort_values(by='PERC_RECS', ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#join fulljoin with pdct_df on SUBCAT_CD_EXT\n","joined_w_description = joined_wo_dsc.merge(pdct_df, on='SUBCAT_CD_EXT')\n","\n","#drop duplicate rows\n","joined_w_description = joined_w_description.drop_duplicates()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Deployment\n","\n","December 2022 does not have the TARGET value. Using the best model/setup used for training, the TARGET value is predicted\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["timestamps_testing = df_200k['fulldate'].sort_values().unique()\n","\n","#check the fulldate \n","#select data from df_200k where full date is the last 3 months\n","X_test_deploy = df_200k[df_200k['fulldate'] >= timestamps_testing[-1]]\n","X_test_deploy = X_test_deploy.drop(columns=['TARGET', 'fulldate'])\n","\n","#select data from df_200k where full date is the second to last month and the third to last month\n","last_3months = df_200k[(df_200k['fulldate'] >= timestamps_testing[-4]) & (df_200k['fulldate'] < timestamps_testing[-1])]\n","\n","#split last_3months into X_train and y_train\n","X_train_deploy = last_3months.drop(columns=['TARGET', 'fulldate'])\n","y_train_deploy = last_3months['TARGET']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def deployment(df):\n","\n","    ### Get the model from the file .joblib\n","    model = joblib.load('LR.joblib')\n","    #print the object type of model\n","    print(type(model))\n","    #select the selected_features from the df\n","    df = df[features_selected]\n","    \n","    result = model.predict(df)\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predicted = deployment(X_test_deploy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predicted.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#compare predicted shape with X_test_deploy shape\n","predicted.shape[0] == X_test_deploy.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#select predicted where value is 1\n","number_of_1 = predicted[predicted == 1].shape\n","number_of_0 = predicted[predicted == 0].shape\n","\n","print(f'Number of recommended \"buys\" for Dec.2022: {number_of_1[0]}')\n","print(f'Number of recommended \"no buys for Dec.2022: {number_of_0[0]}')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPv7tlNcPABoDFliqaQGXet","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}
